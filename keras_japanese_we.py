# 日本語ベクトル化
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
from janome.tokenizer import Tokenizer

# text
example_sentences = [
    "京都の大学",
    "アメリカの美味しい食べ物",
    "機械学習の本",
    "ビル・ゲイツ",
    "御殿場市民"
]

# ベクトル変換
jtok = Tokenizer()

with tf.Graph().as_default():
    embed = hub.Module("https://tfhub.dev/google/nnlm-ja-dim128/1")
    embeddings = embed(list(map(lambda x: ' '.join([y.surface for y in jtok.tokenize(x)]), example_sentences)))


    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.tables_initializer())
        example_vectors = sess.run(embeddings)
        

"""
print('text shape{}'.format(example_vectors.shape))
>>> text shape(5, 128)

# example_vectors[1]の中身

array([ 0.21444249, -0.02066225, -0.02490281,  0.12042393, -0.02669256,
       -0.03639572,  0.0639141 , -0.07621422, -0.09123933,  0.04221497,
       -0.02266158,  0.07067862, -0.0404582 , -0.14392036,  0.02329277,
        0.09088391,  0.02312123,  0.03846002,  0.05741814,  0.00031251,
        0.02235819, -0.23327258,  0.00174309,  0.04039909,  0.01923054,
       -0.20671186,  0.04574473, -0.10783764,  0.15570977,  0.21124859,
        0.23662198,  0.08777227,  0.03669035,  0.02975237, -0.09071992,
       -0.07266812,  0.02674059,  0.03673555, -0.02911181, -0.1486303 ,
        0.02271459,  0.04228514,  0.02575765,  0.01484851, -0.00291231,
       -0.21089311, -0.00445587, -0.21334003, -0.12411128, -0.10119673,
        0.06045113, -0.09723218,  0.08770846, -0.12805086,  0.16502124,
       -0.07979961,  0.2203255 , -0.17222357,  0.01070272,  0.09691209,
       -0.03311934, -0.13294616, -0.14924897, -0.07744226, -0.01559774,
       -0.1402346 ,  0.22744502, -0.07018153,  0.05709712, -0.14845742,
        0.0601044 ,  0.06071291,  0.07477927,  0.02545806, -0.00027584,
        0.04564046, -0.20603304,  0.04277818,  0.07747093,  0.00619286,
        0.14053614, -0.02086988, -0.13657984,  0.03583155, -0.0381945 ,
       -0.15456699, -0.04663824,  0.1366553 , -0.03684065, -0.2111983 ,
       -0.01449677, -0.12352285, -0.03340601,  0.1493544 , -0.11698331,
       -0.04235147, -0.20047963,  0.06850106, -0.00192337,  0.08337143,
        0.0665336 ,  0.06508755, -0.06783675,  0.01749612, -0.02375472,
       -0.04449525, -0.10569633,  0.01875219, -0.0829886 ,  0.03253315,
       -0.01677698,  0.08705967,  0.05160309, -0.06960055, -0.06620288,
       -0.05360216,  0.11966458,  0.01819556,  0.05795261, -0.13429345,
       -0.11908479, -0.0697221 , -0.09247562, -0.02146355,  0.03899785,
       -0.01095748,  0.06306917, -0.01096421], dtype=float32)
"""

# ベクトルのmaxとmin
print(example_vectors.max(), example_vectors.min())     
# max=0.31192207,   min= -0.23901775


# Embeddingレイヤーに入れる時
max=1

model = Sequential()
model.add(Embedding(max, 32))
model.add(LSTM(32))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
history = model.fit(example_vectors, y_train[:5],
                    epochs=5, batch_size=128, validation_split=0.2)